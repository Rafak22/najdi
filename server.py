from openai import OpenAI
from dotenv import load_dotenv
import base64
import os
import logging
import openai_whisper as whisper

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # Try to load from .env file first (for local development)
    load_dotenv()
    logger.info("âœ… Loaded environment variables from .env file")
except Exception as e:
    logger.warning(f"âš ï¸ Could not load .env file: {e}. Assuming Railway env vars are used.")

# Get API key from environment variable
openai_key = os.environ.get("OPENAI_API_KEY")
if not openai_key:
    logger.error("âŒ OPENAI_API_KEY not found in environment variables")
    raise ValueError(
        "OPENAI_API_KEY not found. Please set it in Railway environment variables "
        "or in .env file for local development"
    )

# Initialize OpenAI client with API key from environment
client = OpenAI(api_key=openai_key)
logger.info("âœ… Initialized OpenAI client")

# Load Whisper model for voice transcription
logger.info("ğŸ™ï¸ Loading Whisper model...")
whisper_model = whisper.load_model("base")
logger.info("âœ… Whisper model loaded successfully")

# System prompt for Khaleeji dialect
SAUDI_PROMPT = """
Ø£Ù†Øª Ù†ÙŠÙƒØ³ØªØ§ Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø­ØµØ±ÙŠÙ‹Ø§ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©. Ø¬Ù…ÙŠØ¹ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒØŒ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ«Ù†Ø§Ø¡ØŒ Ù„Ø§Ø²Ù… ØªÙƒÙˆÙ† Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© ÙÙ‚Ø·.

Ù…Ù‡Ø§Ù…Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
1- Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª: Ø¬Ø§ÙˆØ¨ Ø¯ÙˆÙ…Ù‹Ø§ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ÙØµØ­Ù‰ Ø£Ùˆ Ù„Ù‡Ø¬Ø§Øª ØºÙŠØ± Ø®Ù„ÙŠØ¬ÙŠØ©.
2- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø´ÙƒÙ„ Ø°ÙƒÙŠ: Ø®Ù„Ùƒ ÙˆØ§Ø¹ÙŠ Ù„Ù„Ø³ÙŠØ§Ù‚ ÙˆØ®Ù„ Ø¥Ø¬Ø§Ø¨ØªÙƒ ÙˆØ§Ø¶Ø­Ø© ÙˆØ³Ù„Ø³Ø© Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø©.
3- ØªÙƒÙ„Ù… Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¨Ø´Ø±ÙŠ Ø·Ø¨ÙŠØ¹ÙŠ: Ø±Ø¯ Ø¨Ø¹ÙÙˆÙŠØ© ÙˆÙƒØ£Ù†Ùƒ Ø´Ø®Øµ Ø­Ù‚ÙŠÙ‚ÙŠ Ù…Ù† Ø§Ù„Ø®Ù„ÙŠØ¬ ÙŠØªÙƒÙ„Ù… Ù…Ø¹ ØµØ§Ø­Ø¨Ù‡.
4- Ø£Ø¶Ù Ù„Ù…Ø³Ø© ÙˆØ¯ÙŠØ© ÙˆØ£Ø­ÙŠØ§Ù†Ù‹Ø§ Ù†ÙƒÙ‡Ø§Øª Ù…Ø­Ù„ÙŠØ© (Ù…Ø«Ù„: ÙŠØ¨Ù‡ØŒ Ø­Ø¨ÙŠØ¨ÙŠØŒ Ù‡Ù„Ø§ ÙˆØºÙ„Ø§ØŒ Ø§Ù„Ø®).
"""

async def generate_response(message: str):
    """
    Generate a response using OpenAI's GPT-4 and convert it to speech.
    
    Args:
        message (str): The user's input message
        
    Returns:
        dict: A dictionary containing the generated text and audio in base64 format
    """
    try:
        logger.info(f"ğŸ¤– Generating response for message: {message}")
        
        # Prepare messages for chat
        messages = [
            {"role": "system", "content": SAUDI_PROMPT},
            {"role": "user", "content": message}
        ]

        # Generate text response
        response = client.chat.completions.create(
            model="gpt-4",
            messages=messages,
            temperature=0.7,
            max_tokens=150
        )

        generated_text = response.choices[0].message.content
        logger.info(f"âœ… Generated text response: {generated_text}")

        # Generate speech
        logger.info("ğŸ”Š Converting text to speech...")
        speech_response = client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=generated_text
        )

        # Convert audio to base64
        audio_data = speech_response.read()
        audio_base64 = base64.b64encode(audio_data).decode('utf-8')
        logger.info("âœ… Audio conversion complete")

        return {
            "text": generated_text,
            "audio_base64": audio_base64
        }

    except Exception as e:
        logger.error(f"âŒ Error generating response: {str(e)}")
        raise Exception(f"Error generating response: {str(e)}")

async def process_voice_message(audio_file_path: str):
    """
    Process a voice message: transcribe it, generate a response, and convert response to speech.
    
    Args:
        audio_file_path (str): Path to the audio file
        
    Returns:
        dict: A dictionary containing the transcript, reply text, and reply audio
    """
    try:
        logger.info(f"ğŸ™ï¸ Processing voice file: {audio_file_path}")
        
        # Transcribe audio using Whisper
        logger.info("ğŸ” Transcribing audio...")
        result = whisper_model.transcribe(audio_file_path)
        transcript = result["text"]
        logger.info(f"âœ… Transcribed text: {transcript}")

        # Generate response
        logger.info("ğŸ¤– Generating response...")
        response = await generate_response(transcript)
        logger.info("âœ… Response generated successfully")
        
        return {
            "transcript": transcript,
            "reply_text": response["text"],
            "reply_audio": response["audio_base64"]
        }

    except Exception as e:
        logger.error(f"âŒ Error processing voice message: {str(e)}")
        raise Exception(f"Error processing voice message: {str(e)}")